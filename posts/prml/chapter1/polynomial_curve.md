---
layout: distill
permalink: /prml/chapter1/polynomial_curve
title: "Polynomial Curve Fitting"
subtitle: "PRML Chapter 1.1"
date: 2025-04-22
future: true
htmlwidgets: true
hidden: false
giscus_comments: true

previous_section: true
next_section: true
previous_section_url: "/cs4all-vn/prml/"
previous_section_name: "Trang chÃ­nh"
next_section_url: "/cs4all-vn/prml/chapter1/prob_theory"
next_section_name: "Probability Theory"

authors:
  - name: LÃª Nguyá»…n
    url: "https://lenguyen.vercel.app"

toc:
  - name: Giá»›i thiá»‡u bÃ i toÃ¡n
  - name: LÃ m sao Ä‘á»ƒ tÃ¬m mÃ´ hÃ¬nh phÃ¹ há»£p
  - name: Chá»n giÃ¡ trá»‹ hyperparameter
---

## Giá»›i thiá»‡u bÃ i toÃ¡n

Giáº£ sá»­ báº¡n An cÃ³ sá»‘ tiá»n lÃ  $x \in \mathbb{R}$ vÃ  LN muá»‘n dá»± Ä‘oÃ¡n xem vá»›i sá»‘ tiá»n $x$ nÃ y, An cÃ³ thá»ƒ mua Ä‘Æ°á»£c bao nhiÃªu cÃ¡i bÃ¡nh xÃ¨o, bá»Ÿi vÃ¬ bÃ  bÃ¡n bÃ¡nh xÃ¨o báº£ khÃ´ng muá»‘n tiáº¿t lá»™ giÃ¡ 1 cÃ¡i bÃ¡nh xÃ¨o vÃ  giÃ¡ cÃ³ thá»ƒ thay Ä‘á»•i má»—i ngÃ y (nhÆ°ng bÃ¡nh xÃ¨o ngon). Gá»i sá»‘ bÃ¡nh xÃ¨o mua Ä‘Æ°á»£c lÃ  $y \in \mathbb{R}$ (trong thá»±c táº¿ sá»‘ bÃ¡nh sáº½ lÃ  sá»‘ nguyÃªn, nhÆ°ng mÃ  mÃ¬nh giáº£ sá»­ bÃ  bÃ¡n bÃ¡nh cÃ³ kháº£ nÄƒng nÃ o Ä‘Ã³ bÃ¡n Ä‘Æ°á»£c sá»‘ bÃ¡nh xÃ¨o lÃ  sá»‘ thá»±c).

<p markdown=1 class="takeaway">BÃ i toÃ¡n dÃ¹ng má»™t giÃ¡ trá»‹ Ä‘áº§u vÃ o $$x$$ Ä‘á»ƒ dá»± Ä‘oÃ¡n má»™t giÃ¡ trá»‹ sá»‘ thá»±c $$y$$ nÃ o Ä‘Ã³ Ä‘Æ°á»£c gá»i lÃ  bÃ i toÃ¡n **há»“i quy** (regression).</p>

Giáº£ sá»­ An thu tháº­p dá»¯ liá»‡u cho viá»‡c dá»± Ä‘oÃ¡n cá»§a mÃ¬nh thÃ´ng qua $N$ láº§n mua, Ä‘áº·t lÃ  $\mathbf{x} \equiv (x_1, ..., x_N)^T$. Vá»›i má»—i $x_i$, An biáº¿t Ä‘Æ°á»£c sá»‘ bÃ¡nh xÃ¨o mua Ä‘Æ°á»£c lÃ  $y_i$, váº­y ta cÃ³ $\mathbf{y} \equiv (y_1, ..., y_N)^T$. Ta gá»i dá»¯ liá»‡u mÃ  An thu tháº­p Ä‘Æ°á»£c $\{ (x_1, y_1), ..., (x_N, y_N) \}$ lÃ  **táº­p huáº¥n luyá»‡n** (training set). Dá»±a trÃªn táº­p huáº¥n luyá»‡n áº¥y, An muá»‘n dá»± Ä‘oÃ¡n giÃ¡ trá»‹ $\hat{y}$ cho má»™t giÃ¡ trá»‹ $\hat{x}$ má»›i (láº§n mua má»›i). VÃ¬ váº­y An cáº§n tÃ¬m ra má»™t mÃ´ hÃ¬nh (hay quy luáº­t) nÃ o Ä‘Ã³, Ä‘á»ƒ khi Ä‘Æ°a vÃ o giÃ¡ trá»‹ $\hat{x}$ nÃ o Ä‘Ã³ (gá»i lÃ  input), mÃ´ hÃ¬nh cho ra Ä‘Æ°á»£c giÃ¡ trá»‹ $\hat{y}$ phÃ¹ há»£p (gá»i lÃ  output).

An Ä‘oÃ¡n ráº±ng, viá»‡c dÃ¹ng má»™t hÃ m Ä‘a thá»©c nhÆ° dÆ°á»›i cÃ³ thá»ƒ dá»± Ä‘oÃ¡n Ä‘Æ°á»£c sá»‘ bÃ¡nh:

$$
f(x, \mathbf{w}) = w_{0} + w_{1}x + w_{2}x^2 + \dots w_{M}x^M = \sum_{i=0}^M w_{i}x^i
$$

trong Ä‘Ã³ $\mathbf{w} = (w_0, w_1, ..., w_M)^T$ lÃ  bá»™ tham sá»‘ cá»§a hÃ m Ä‘a thá»©c $f(x, \mathbf{w})$ mÃ  An chá»n vÃ  $M$ lÃ  **báº­c** (degree) cá»§a Ä‘a thá»©c áº¥y. Váº­y cÃ³ hai thá»© mÃ  ta cáº§n Ä‘á»ƒ Ã½ khi dÃ¹ng Ä‘a thá»©c (hay mÃ´ hÃ¬nh) nÃ y, Ä‘Ã³ chÃ­nh lÃ  $M$ vÃ  $\mathbf{w}$, Ä‘á»ƒ tÃ¬m Ä‘Æ°á»£c mÃ´ hÃ¬nh phÃ¹ há»£p vá»›i dá»¯ liá»‡u Ä‘Ã£ cÃ³, ta cáº§n tÃ¬m $\mathbf{w}$ vÃ  $M$ phÃ¹ há»£p.

<p markdown=1 class="takeaway">Máº·c dÃ¹ $f(x, \mathbf{w})$ lÃ  má»™t hÃ m khÃ´ng tuyáº¿n tÃ­nh cá»§a $x$ nhÆ°ng láº¡i lÃ  má»™t hÃ m tuyáº¿n tÃ­nh cá»§a $\mathbf{w}$, vÃ¬ váº­y ta cÃ³ thá»ƒ xem bÃ i toÃ¡n nÃ y lÃ  má»™t bÃ i toÃ¡n **há»“i quy tuyáº¿n tÃ­nh** (linear regression) bá»Ÿi vÃ¬ Ä‘Ã¢y lÃ  bÃ i toÃ¡n regression vÃ  ta dÃ¹ng mÃ´ hÃ¬nh linear ğŸ¤¯.</p>

## LÃ m sao Ä‘á»ƒ tÃ¬m mÃ´ hÃ¬nh phÃ¹ há»£p

Äá»ƒ tÃ¬m Ä‘Æ°á»£c $\mathbf{w}$ phÃ¹ há»£p vá»›i dá»¯ liá»‡u, ta cáº§n má»™t hÃ m Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ xem, liá»‡u giÃ¡ trá»‹ $\mathbf{w}$ mÃ  ta chá»n **tá»‘t hay lÃ  khÃ´ng** (khÃ´ng tá»‘t thÃ¬ chá»n láº¡i, chá»n nÃ o tá»‘t thÃ¬ thÃ´i ğŸ‘Œ). HÃ m Ä‘Ã¡nh giÃ¡ Ä‘Ã³ Ä‘Æ°á»£c gá»i lÃ  **hÃ m lá»—i** (error function), kÃ­ hiá»‡u $E(\mathbf{w})$, $E(\mathbf{w})$ cÃ ng tháº¥p (lá»—i cÃ ng Ã­t) thÃ¬ $\mathbf{w}$ cÃ ng phÃ¹ há»£p vá»›i dá»¯ liá»‡u, do Ä‘Ã³ má»¥c tiÃªu tá»‘i thÆ°á»£ng ğŸ˜¤ lÃ  pháº£i tÃ¬m giÃ¡ trá»‹ $\mathbf{w}^\star$ sao cho $E(\mathbf{w}^\star)$ lÃ  nhá» nháº¥t.

Vá»›i má»—i loáº¡i bÃ i toÃ¡n khÃ¡c nhau thÃ¬ ta sáº½ chá»n error function khÃ¡c nhau. Äá»ƒ cho dá»…, mÃ¬nh sáº½ chá»n hÃ m lá»—i nhÆ° sau:

$$
E(\mathbf{w}) = \frac{1}{2} \sum_{i=1}^N [f(x_{i}, \mathbf{w}) - y_{i}]^2
$$

ta cÃ³ thá»ƒ hiá»ƒu hÃ m lá»—i nÃ y lÃ  tá»•ng bÃ¬nh phÆ°Æ¡ng Ä‘á»™ lá»—i (hay Ä‘á»™ khÃ¡c nhau) giá»¯a sá»‘ bÃ¡nh dá»± Ä‘oÃ¡n $f(x_i, \mathbf{w})$ vÃ  sá»‘ bÃ¡nh thá»±c sá»± mua Ä‘Æ°á»£c $y_i$. NgoÃ i ra $E(\mathbf{w}) = 0$ khi vÃ  chá»‰ khi $f(x_{i}, \mathbf{w}) = y_{i} \hspace{3pt} \forall i = 1\dots N$, nghÄ©a lÃ  khÃ´ng cÃ³ lá»—i nÃ o á»Ÿ Ä‘Ã¢y, $f(x, \mathbf{w})$ *khá»›p* (fit) hoÃ n toÃ n vá»›i dá»¯ liá»‡u.

Vá»›i hÃ m lá»—i nhÆ° trÃªn, ta hoÃ n toÃ n cÃ³ thá»ƒ tÃ¬m Ä‘Æ°á»£c giÃ¡ trá»‹ $\mathbf{w}^\star$ bá»Ÿi vÃ¬ $f(x_i, \mathbf{w})$ lÃ  má»™t hÃ m tuyáº¿n tÃ­nh nÃªn $[f(x_{i}, \mathbf{w}) - y_{i}]^2$ lÃ  má»™t hÃ m báº­c 2, do Ä‘Ã³ Ä‘áº¡o hÃ m cá»§a $E(\mathbf{w})$ theo $w_i$ láº¡i lÃ  má»™t hÃ m tuyáº¿n tÃ­nh, vÃ¬ váº­y (gáº§n nhÆ°) tá»“n táº¡i má»™t nghiá»‡m duy nháº¥t vÃ  nghiá»‡m sáº½ lÃ  giÃ¡ trá»‹ mÃ  ta cáº§n tÃ¬m.

$$
\begin{align}
\dfrac{\partial E(\mathbf{w})}{\partial \mathbf{w}} = \begin{bmatrix}
\dfrac{\partial{E(\mathbf{w})}}{\partial w_{0}} \dots 
\dfrac{\partial{E(\mathbf{w})}}{\partial w_{N}}
\end{bmatrix}^T &= 0 \\
\implies \dfrac{\partial E(\mathbf{w})}{\partial w_{i}} = 0 \hspace{5pt} \forall i=0 ... N
\end{align}
$$

VÃ¬ váº­y, káº¿t quáº£ mÃ  thoáº£ mÃ£n phÆ°Æ¡ng trÃ¬nh trÃªn, gá»i lÃ  $$\mathbf{w}^\star$$, lÃ  giÃ¡ trá»‹ lÃ m cho error function nhá» nháº¥t. Váº­y váº¥n Ä‘á» tiáº¿p theo lÃ  cáº§n tÃ¬m $$M$$ phÃ¹ há»£p ná»¯a thÃ´i. NgoÃ i ra $$M$$ khÃ´ng náº±m trong bá»™ tham sá»‘ $$\mathbf{w}$$ cá»§a mÃ´ hÃ¬nh nÃªn ta cÃ²n cÃ³ thá»ƒ gá»i $$M$$ lÃ  **hyperparameter** (siÃªu tham sá»‘) cá»§a mÃ´ hÃ¬nh.

### Chá»n giÃ¡ trá»‹ hyperparameter 

NOTE: Pháº§n nÃ y mÃ¬nh nghÄ© cÃ¡c báº¡n nÃªn tá»± Ä‘á»c, khÃ¡ hay vÃ  nhiá»u insight, má»™t pháº§n vÃ¬ mÃ¬nh cÅ©ng lÆ°á»i ğŸ˜¿.
