---
layout: distill
permalink: /prml/chapter1/prob_theory/normal
title: "Gaussian Distribution"
subtitle: "PRML Chapter 1.2.4"
date: 2025-04-23

future: true
htmlwidgets: true
hidden: false

giscus_comments: true

bibliography: main.bib

previous_section: true
next_section: true

previous_section_url: "/cs4all-vn/prml/chapter1/prob_theory/bayes"
previous_section_name: "Bayesian Probabilities"
next_section_url: "/cs4all-vn/prml/chapter1/prob_theory/curve_revisit"
next_section_name: "Curve Fitting Revisited"


authors:
  - name: L√™ Nguy·ªÖn
    url: "https://lenguyen.vercel.app"

toc:
  - name: L√Ω thuy·∫øt
  - name: Ch·ª©ng minh trung b√¨nh v√† ph∆∞∆°ng sai m·∫´u
---

## L√Ω thuy·∫øt

<p markdown=1 class="definition">
**Ph√¢n ph·ªëi chu·∫©n** (Gaussian Distribution ho·∫∑c Normal Distribution), k√≠ hi·ªáu l√† $\mathcal{N}(x \mid \mu, \sigma^2)$, s·∫Ω ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ sau:
\\[
\mathcal{N}(x \mid \mu, \sigma^2) = \frac{1}{(2\pi \sigma^2)^{1/2}} \exp \left\\{ - \frac{1}{2\sigma^2} (x - \mu)^2 \right\\}
\\]
V·ªõi $x$ l√† s·ªë th·ª±c v√† $\mathcal{N}(x \mid \mu, \sigma^2)$ c√≥ nghƒ©a l√† ph√¢n ph·ªëi g·ªìm 2 tham s·ªë l√† $\mu$ v√† $\sigma^2$, trong ƒë√≥ $\mu$ l√† **trung b√¨nh** (mean) c·ªßa ph√¢n ph·ªëi, $\sigma^2$ l√† **ph∆∞∆°ng sai** (variance) c·ªßa ph√¢n ph·ªëi.
</p>

Ngo√†i ra, n·∫øu l·∫•y cƒÉn c·ªßa ph∆∞∆°ng sai, ta ƒë∆∞·ª£c $\sigma$ v√† ta g·ªçi gi√° tr·ªã ƒë√≥ l√† **ƒë·ªô l·ªách chu·∫©n** (standard deviation) c·ªßa ph√¢n ph·ªëi. C√≤n n·∫øu l·∫•y ngh·ªãch ƒë·∫£o c·ªßa ph∆∞∆°ng sai v√† ƒë·∫∑t gi√° tr·ªã ƒë√≥ l√† $\beta$, t·ª©c l√† $\beta = 1/(\sigma^2)$, ta g·ªçi $\beta$ l√† **ƒë·ªô ch√≠nh x√°c** (precision) c·ªßa ph√¢n ph·ªëi.

<p markdown=1 class="takeaway">
Khi ta n√≥i m·ªôt bi·∫øn ng·∫´u nhi√™n li√™n t·ª•c $X$ n√†o ƒë√≥ c√≥ ph√¢n ph·ªëi $f$ v·ªõi c√°c tham s·ªë $\theta_{i}$, ta k√≠ hi·ªáu $X \sim f(\theta_{1}, \theta_{2}, \dots)$. V√≠ d·ª•, $X$ c√≥ ph√¢n ph·ªëi chu·∫©n v·ªõi trung b√¨nh l√† $\mu$ v√† ph∆∞∆°ng sai l√† $\sigma^2$ th√¨ ta vi·∫øt $X \sim \mathcal{N}(\mu, \sigma^2)$. Ngo√†i ra khi vi·∫øt $X$ c√≥ ph√¢n ph·ªëi, ta ng·∫ßm hi·ªÉu ph√¢n ph·ªëi ƒë√≥ l√† m·∫≠t ƒë·ªô x√°c su·∫•t (pdf) (ƒë·ªëi v·ªõi bi·∫øn li√™n t·ª•c) c·ªßa $X$.
</p>

{% include figure.liquid path="https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/2560px-Normal_Distribution_PDF.svg.png" class="img-fluid" caption="H√¨nh 1: ƒê·ªì th·ªã c·ªßa ph√¢n ph·ªëi chu·∫©n, c√≥ th·ªÉ th·∫•y ƒë·ªì th·ªã c√≥ d·∫°ng th√°p chu√¥ng (ngu·ªìn: Wikipedia)" %}

Ta c√≥ k√¨ v·ªçng c·ªßa m·ªôt bi·∫øn ng·∫´u nhi√™n $X \sim \mathcal{N}(\mu, \sigma^2)$ l√†:

$$
\mathbb{E}[X] = \int_{-\infty}^{\infty} \mathcal{N}(x \mid \mu, \sigma^2)x dx = \mu
$$

V·∫≠y k√¨ v·ªçng c·ªßa $X$ ch√≠nh l√† gi√° tr·ªã trung b√¨nh c·ªßa ph√¢n ph·ªëi. Ngo√†i ra, ta c√≥:

$$
\mathbb{E}[X^2] = \int_{-\infty}^{\infty} \mathcal{N}(x \mid \mu, \sigma^2)x^2 dx = \mu^2 + \sigma^2
$$

Ta g·ªçi gi√° tr·ªã $\mathbb{E}[X^2]$ l√† **moment b·∫≠c 2** (second order moment) c·ªßa $X$. T∆∞∆°ng t·ª± moment b·∫≠c $k$ c·ªßa $X$ s·∫Ω l√† $\mathbb{E}[X^k]$. T·ª´ hai ph∆∞∆°ng tr√¨nh tr√™n, ta c√≥ ph∆∞∆°ng sai c·ªßa $X$ l√†:

$$
\text{var}[X] = \mathbb{E}[X^2] - \mathbb{E}[X]^2 = \sigma^2
$$

Gi√° tr·ªã l·ªõn nh·∫•t $x$ l√†m cho ph√¢n ph·ªëi c·ª±c ƒë·∫°i c√≤n ƒë∆∞·ª£c g·ªçi l√† **mode** v√† ph√¢n ph·ªëi chu·∫©n c√≥ $mode = \mu$, t·ª©c l√†:

$$
\text{arg}\max_{x} \mathcal{N}(x \mid \mu, \sigma^2) = \mu
$$

<p markdown=1 class="takeaway">
C√°c c√¥ng th·ª©c tr√™n ƒë·ªÅu ƒë∆∞·ª£c ch·ª©ng minh ·ªü ph·∫ßn b√†i t·∫≠p.
</p>

X√©t m·ªôt vector $D$ chi·ªÅu g·ªìm c√°c s·ªë th·ª±c $\mathbf{x} =(x_{1}, \dots, x_{D})^T$. Ta ƒë·ªãnh nghƒ©a ph√¢n ph·ªëi chu·∫©n tr√™n vector $\mathbf{x}$ l√†:

$$
\mathcal{N}(\mathbf{x} \mid \pmb{\mu}, \pmb{\Sigma}) = \frac{1}{(2\pi)^{D/2}} \frac{1}{|\pmb{\Sigma}|^{1/2}} \exp \left\{ -\frac{1}{2} (\mathbf{x} - \pmb{\mu})^T \pmb{\Sigma}^{-1} (\mathbf{x} - \pmb{\mu}) \right\}
$$

Trong ƒë√≥ $\pmb{\mu}$ l√† vector trung b√¨nh c·ªßa ph√¢n ph·ªëi c√≥ $D$ chi·ªÅu, $\pmb{\Sigma}$ l√† ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai c√≥ k√≠ch th∆∞·ªõc $D \times D$ v√† $\mid\pmb{\Sigma}\mid$ l√† ƒë·ªãnh th·ª©c c·ªßa ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai $\pmb{\Sigma}$. M·ªôt t√™n g·ªçi kh√°c cho ph√¢n ph·ªëi chu·∫©n nhi·ªÅu chi·ªÅu l√† **multivariate normal (ho·∫∑c gaussian) distribution**.

Gi·∫£ s·ª≠ ta c√≥ m·ªôt t·∫≠p d·ªØ li·ªáu $\mathcal{D} = \lbrace x_{1}, \dots, x_{N}\rbrace$. T·∫≠p d·ªØ li·ªáu bao g·ªìm $N$ quan s√°t, m·ªói quan s√°t (observation) l√† m·ªôt ƒë·∫°i l∆∞·ª£ng v√¥ h∆∞·ªõng (scalar) $x_{i}$.

<p class="takeaway" markdown=1>
Ta g·ªçi m·ªôt gi√° tr·ªã $x$ l√† scalar n·∫øu n√≥ kh√¥ng ph·∫£i l√† vector (ez huh). ƒê√∫ng h∆°n, scalar (hay ƒë·∫°i l∆∞·ª£ng v√¥ h∆∞·ªõng) ƒë·ªÉ ch·ªâ ph·∫ßn t·ª≠ c·ªßa m·ªôt tr∆∞·ªùng (field) ([Scalar (mathematics) - Wikipedia](https://en.wikipedia.org/wiki/Scalar_(mathematics))). T·∫≠p s·ªë th·ª±c ($\mathbb{R}$) l√† m·ªôt tr∆∞·ªùng, do ƒë√≥ ta c√≥ th·ªÉ n√≥i c√°c s·ªë th·ª±c $x \in \mathbb{R}$ l√† m·ªôt ƒë·∫°i l∆∞·ª£ng v√¥ h∆∞·ªõng. Ngo√†i ra, t·∫≠p s·ªë ph·ª©c $\mathbb{C}$ c≈©ng l√† m·ªôt tr∆∞·ªùng n√™n $x$ c≈©ng c√≥ th·ªÉ l√† s·ªë ph·ª©c n·∫øu ta ch·ªâ n√≥i $x$ l√† ƒë·∫°i l∆∞·ª£ng v√¥ h∆∞·ªõng m√† kh√¥ng n√≥i g√¨ th√™m.
</p>

Gi·∫£ s·ª≠ c√°c quan s√°t trong t·∫≠p d·ªØ li·ªáu $\mathcal{D}$ c·ªßa ta ƒë∆∞·ª£c l·∫•y ra m·ªôt c√°ch ƒë·ªôc l·∫≠p (drawn independently) t·ª´ m·ªôt ph√¢n ph·ªëi chu·∫©n c√≥ trung b√¨nh l√† $\mu$ v√† ph∆∞∆°ng sai l√† $\sigma^2$ (ƒë√¢y l√† hai ƒë·∫°i l∆∞·ª£ng m√† ta ch∆∞a bi·∫øt v√† m·ª•c ƒë√≠ch c·ªßa ch√∫ng ta l√† t√¨m ra ƒë∆∞·ª£c hai tham s·ªë n√†y t·ª´ t·∫≠p d·ªØ li·ªáu m√† ta c√≥).

C√°c ƒëi·ªÉm d·ªØ li·ªáu m√†:
- ƒê∆∞·ª£c l·∫•y ra t·ª´ c√πng m·ªôt ph√¢n ph·ªëi (identically distributed).
- ƒê·ªôc l·∫≠p v·ªõi nhau (independent).

th√¨ ƒë∆∞·ª£c n√≥i l√† **ƒë·ªôc l·∫≠p v√† c√≥ ph√¢n ph·ªëi ƒë·ªìng nh·∫•t** (independent and identically distributed) v√† th∆∞·ªùng vi·∫øt t·∫Øt l√† i.i.d.

<p class="takeaway" markdown=1>
X√©t t·∫≠p d·ªØ li·ªáu $\mathcal{D}$, n·∫øu ta vi·∫øt $\mathcal{x_i} \overset{i.i.d}{\sim} \mathcal{N}(\mu, \sigma^2), i = 1...N$ t·ª©c l√† t·∫≠p d·ªØ li·ªáu $\mathcal{D}$ l√† ƒë·ªôc l·∫≠p v√† c√≥ ph√¢n ph·ªëi ƒë·ªìng nh·∫•t, ngo√†i ra $\mathcal{D}$ ƒë∆∞·ª£c l·∫•y ra t·ª´ ph√¢n ph·ªëi chu·∫©n.
</p>

X√©t h√†m likelihood $\mathcal{L}(\mu, \sigma^2 \mid \mathcal{D})$, b·ªüi v√¨ $\mathcal{D}$ l√† i.i.d n√™n ta c√≥:

$$
\mathcal{L}(\mu, \sigma^2 \mid \mathcal{D}) = p(\mathcal{D} \mid \mu, \sigma^2) = p(x_{1}, \dots, x_{N} \mid \mu, \sigma^2) = \prod_{n=1}^N p(x_{n} \mid \mu, \sigma^2)
$$

M·ªôt trong nh·ªØng c√°ch th∆∞·ªùng d√πng ƒë·ªÉ t√¨m c√°c tham s·ªë cho ph√¢n ph·ªëi b·∫±ng c√°ch s·ª≠ d·ª•ng t·∫≠p d·ªØ li·ªáu quan s√°t ƒë∆∞·ª£c ($\mathcal{D}$) l√† t√¨m c√°c tham s·ªë m√† **l√†m c·ª±c ƒë·∫°i** h√†m likelihood (hay c√≤n g·ªçi l√† **maximum likelihood**). Hay n√≥i c√°ch kh√°c:

$$
\hat{\mu}, \hat{\sigma}^2 = \text{arg}\max_{\mu, \sigma^2} \mathcal{L}(\mu, \sigma^2 \mid \mathcal{D})
$$

<p class="takeaway" markdown=1>
K√≠ hi·ªáu $\displaystyle \text{arg}\max_{x} f(x)$ c√≥ nghƒ©a l√† gi√° tr·ªã $x$ sao cho $f(x)$ l√† l·ªõn nh·∫•t (c·ª±c ƒë·∫°i).
</p>

ƒê·ªÖ d·ªÖ d√†ng h∆°n, thay v√¨ t√¨m c√°c tham s·ªë l√†m c·ª±c ƒë·∫°i h√†m likelihood, ta t√¨m c√°c tham s·ªë l√†m c·ª±c ƒë·∫°i h√†m log (log ·ªü ƒë√¢y s·∫Ω ƒë∆∞·ª£c hi·ªÉu l√† $\ln$) c·ªßa h√†m likelihood, nghƒ©a l√†:

$$
\hat{\mu}, \hat{\sigma}^2 = \text{arg}\max_{\mu, \sigma^2} \ln \mathcal{L}(\mu, \sigma^2 \mid \mathcal{D})
$$

Do ƒë√≥, ta c√≥ th·ªÉ vi·∫øt h√†m likelihood l·∫°i nh∆∞ sau:

$$
\begin{aligned}
\ln \mathcal{L}(\mu, \sigma^2 \mid \mathcal{D}) &= \ln \prod_{n=1}^N p(x_{n} \mid \mu, \sigma^2) \\
&= \left[-\frac{1}{2\sigma^2} \sum_{n=1}^N (x_{n} - \mu)^2 \right] - \frac{N}{2} \ln 2\pi - \frac{N}{2}\ln \sigma^2
\end{aligned}
$$

C·ª±c ƒë·∫°i h√†m log likelihood ph√≠a tr√™n theo $\mu$, ta c√≥:

$$
\mu_{ML} = \frac{1}{N} \sum_{n=1}^N x_{n}
$$

trong ƒë√≥ $\mu_{ML}$ ƒë∆∞·ª£c g·ªçi l√† **trung b√¨nh m·∫´u** (sample mean) t·ª©c l√† trung b√¨nh c·ªßa c√°c quan s√°t $\{x_n\}$ m√† ta quan s√°t ƒë∆∞·ª£c. C√≤n n·∫øu ta c·ª±c ƒë·∫°i theo $\sigma^2$, ta c√≥:

$$
\sigma^2_{ML} = \frac{1}{N} \sum_{n=1}^N (x_{n} - \mu_{ML})^2
$$

Ta g·ªçi $\sigma_{ML}^2$ l√† **ph∆∞∆°ng sai m·∫´u** (sample variance), ngo√†i ra ta th·∫•y $\sigma_{ML}^2$ c≈©ng ph·ª• thu·ªôc v√†o $\mu_{ML}$. V·ªÅ l√Ω thuy·∫øt l√† ta c·∫ßn t√≠nh c·∫£ hai c√πng l√∫c (t√¨m b·ªô tham s·ªë l√†m c·ª±c ƒë·∫°i, m√† b·ªô tham s·ªë g·ªìm $n$ bi·∫øn th√¨ t√¨m c√πng l√∫c $n$ bi·∫øn) th·∫ø nh∆∞ng trong tr∆∞·ªùng h·ª£p n√†y, $\mu_{ML}$ kh√¥ng ph·ª• thu·ªôc v√†o $\sigma_{ML}^2$ do ƒë√≥ ta c√≥ th·ªÉ t√¨m $\mu_{ML}$ tr∆∞·ªõc sau ƒë√≥ t√¨m $\sigma_{ML}^2$.

<p class="takeaway" markdown=1>
Th√¥ng th∆∞·ªùng gi√° tr·ªã trung b√¨nh $\mu$ ƒë∆∞·ª£c g·ªçi trung b√¨nh t·ªïng th·ªÉ (population mean) t∆∞∆°ng t·ª± v·ªõi $\sigma^2$ l√† ph∆∞∆°ng sai t·ªïng th·ªÉ (population variance), ƒë√¢y l√† gi√° tr·ªã m√† ta kh√¥ng bi·∫øt, th·∫ø nh∆∞ng b·∫±ng c√°ch d√πng m·ªôt ph·∫ßn c·ªßa t·ªïng th·ªÉ (g·ªçi l√† m·∫´u), ta s·∫Ω c·ªë g·∫Øng ∆∞·ªõc l∆∞·ª£ng ƒë∆∞·ª£c gi√° tr·ªã $\mu$ v·ªõi $\sigma^2$ t·ªët nh·∫•t. Nh∆∞ ƒë√£ ch·ª©ng minh ph√≠a tr√™n, gi√° tr·ªã ∆∞·ªõc l∆∞·ª£ng t·ªët nh·∫•t ch√≠nh l√† $\mu_{ML}$ (trung b√¨nh c·ªßa m·∫´u) v√† $\sigma^2_{ML}$ (ph∆∞∆°ng sai c·ªßa m·∫´u).
</p>

X√©t gi√° tr·ªã k√¨ v·ªçng c·ªßa trung b√¨nh m·∫´u $\mu_{ML}$, ta c√≥:

$$
\mathbb{E}[\mu_{ML}] = \mu
$$

C√≥ th·ªÉ th·∫•y, k√¨ v·ªçng c·ªßa trung b√¨nh m·∫´u $\mu_{ML}$ ch√≠nh l√† trung b√¨nh c·ªßa ph√¢n ph·ªëi $\mu$, ƒë√∫ng nh∆∞ ta d·ª± ƒëo√°n, gi√° tr·ªã $\mu_{ML}$ c√≥ th·ªÉ ƒë∆∞·ª£c d√πng ∆∞·ªõc l∆∞·ª£ng r·∫•t t·ªët $\mu$. Th·∫ø nh∆∞ng, n·∫øu x√©t gi√° tr·ªã k√¨ v·ªçng c·ªßa ph∆∞∆°ng sai m·∫´u $\sigma^2_{ML}$, ta c√≥:

$$
\mathbb{E}[\sigma^2_{ML}] = \frac{(N-1)}{N} \sigma^2
$$

M·∫∑c d√π ∆∞·ªõc l∆∞·ª£ng t·ªët v·ªõi $\mu_{ML}$ th·∫ø nh∆∞ng $\sigma_{ML}^2$ th√¨ kh√¥ng. D√πng $\sigma_{ML}^2$ ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng cho $\sigma^2$ th√¨ cho ra gi√° tr·ªã th·∫•p h∆°n, ta g·ªçi c√°ch ∆∞·ªõc l∆∞·ª£ng n√†y l√† **ƒë√°nh gi√° th·∫•p** (underestimate) (hay c√≤n g·ªçi l√† bias). ƒê·ªÉ tr√°nh vi·ªác bias nh∆∞ n√†y, ta ch·ªâ c·∫ßn chia cho $N-1$ thay v√¨ $N$ ·ªü ph∆∞∆°ng sai m·∫´u:

$$
\begin{aligned}
\sigma^2_{ML} &= \frac{1}{N-1} \sum_{n=1}^N (x_{n} - \mu_{ML})^2 \\
\implies \mathbb{E}[\sigma^2_{ML}] &= \sigma^2
\end{aligned}
$$

V√† ƒë√¢y l√† l√Ω do m√† ng∆∞·ªùi ta th∆∞·ªùng chia cho $N-1$ thay v√¨ $N$ ·ªü ph∆∞∆°ng sai m·∫´u.

Th·∫ø nh∆∞ng khi $N$ tr·ªü l√™n l·ªõn d·∫ßn, vi·ªác bias c·ªßa nghi·ªám c·ªßa maximum likelihood ($\sigma_{ML}^2$) kh√¥ng c√≤n qu√° quan tr·ªçng n·ªØa (v√≠ d·ª• b·∫°n c√≥ $N = 100001$ th√¨ $N - 1 = 100000$ s·∫Ω cho ra k·∫øt qu·∫£ kh√¥ng qu√° ch√™nh l·ªách). Khi m√† $N \to \infty$ th√¨ ph∆∞∆°ng sai m·∫´u $\sigma_{ML}^2$ s·∫Ω ti·∫øn d·∫ßn v·ªÅ ph∆∞∆°ng sai th·ª±c s·ª± $\sigma$ c·ªßa ph√¢n ph·ªëi. Trong th·ª±c t·∫ø, n·∫øu $N$ kh√¥ng nh·ªè th√¨ bias kh√¥ng ph·∫£i l√† m·ªôt v·∫•n ƒë·ªÅ quan tr·ªçng l·∫Øm.

<p class="takeaway" markdown=1>
Vi·ªác ph∆∞∆°ng sai m·∫´u $\sigma^2_{ML}$ ti·∫øn d·∫ßn v·ªÅ ph∆∞∆°ng sai th·ª±c s·ª± $\sigma$ c·ªßa ph√¢n ph·ªëi khi m√† $N \to \infty$ ƒë∆∞·ª£c ch·ª©ng minh c·ª• th·ªÉ ·ªü **lu·∫≠t s·ªë l·ªõn** (Law of Large Number) <d-footnote>Law of large numbers - Wikipedia (https://en.wikipedia.org/wiki/Law_of_large_numbers)</d-footnote>.
</p>

Tuy nhi√™n v·ªõi c√°c m√¥ h√¨nh ML ph·ª©c t·∫°p c√≥ nhi·ªÅu tham s·ªë th√¨ v·∫•n ƒë·ªÅ bias n√†y l·∫°i tr·ªü n√™n nghi√™m tr·ªçng. ·ªû c√°c ph·∫ßn sau, t√°c gi·∫£ s·∫Ω cho th·∫•y v·∫•n ƒë·ªÅ bias c·ªßa maximum likelihood l√† m·ªôt trong nh·ªØng nguy√™n nh√¢n g√¢y ra over-fitting.

## Ch·ª©ng minh trung b√¨nh v√† ph∆∞∆°ng sai m·∫´u

<p class="takeaway"> Ph·∫ßn n√†y optional, m√¨nh ch·ª©ng minh ch·ªâ ƒë·ªÉ hi·ªÉu h∆°n th√¥i </p>

X√©t 1 m·∫´u $\mathcal{D}$ g·ªìm $N$ quan s√°t ${} x_{1}, \dots, x_{N} {}$ v√† $\mathcal{D} \overset{i.i.d}{\sim} \mathcal{N}(\mu, \sigma^2)$. ƒê·∫∑t $\mu_{ML}$ l√† trung b√¨nh m·∫´u v√† $\sigma^2_{ML}$ l√† ph∆∞∆°ng sai m·∫´u.

### Trung b√¨nh m·∫´u

Ta c√≥:

$$
\begin{aligned}
\mathbb{E}[\mu_{ML}] &= \mathbb{E}\left[ \frac{1}{N} \sum_{n=1}^N x_{n} \right] \\
&= \frac{1}{N} \sum_{n=1}^N \mathbb{E}[x_{n}] \\
&= \frac{1}{N} \sum_{n=1}^N \mu \\
&= \mu
\end{aligned}
$$

### Ph∆∞∆°ng sai m·∫´u

Ta c√≥:

$$
\begin{aligned}
\mathbb{E}[\sigma^2_{ML}] &= \mathbb{E}\left[ \frac{1}{N} \sum_{n=1}^N (x_{n} - \mu_{ML})^2 \right] \\
&= \frac{1}{N} \sum_{n=1}^N \mathbb{E}[x_{n}^2 -2x_{n}\mu_{ML} + \mu_{ML}^2] \\
&= \frac{1}{N} \sum_{n=1}^N \mathbb{E}[x_{n}^2] -2\mathbb{E}[x_{n}\mu_{ML}] + \mathbb{E}[\mu_{ML}^2] \\
\end{aligned}
$$

Nh∆∞ ƒë√£ bi·∫øt ·ªü tr∆∞·ªõc ƒë√≥ th√¨ moment b·∫≠c 2 c·ªßa ph√¢n ph·ªëi chu·∫©n s·∫Ω c√≥ gi√° tr·ªã l√† $\mathbb{E}[X^2] = \mu^2 + \sigma^2$. C√≤n gi√° tr·ªã $\mathbb{E}[x_n\mu_{ML}]$ s·∫Ω ƒë∆∞·ª£c t√≠nh nh∆∞ sau (nh·ªõ l√† c√°c quan s√°t ƒë·ªôc l·∫≠p v·ªõi nhau, do ƒë√≥ v·ªõi hai quan s√°t $x_i$ v√† $x_j$ b·∫•t k√¨, ta c√≥ $\mathbb{E}[x_ix_j] = \mathbb{E}[x_i]\mathbb{E}[x_j]$):

$$
\begin{aligned}
\mathbb{E}[x_{n}\mu_{ML}] &= \mathbb{E}\left[ \frac{1}{N} \sum_{i=1}^N x_n x_{i} \right] \\
&= \frac{1}{N} \left[ \sum_{i \neq n} \mathbb{E}[x_{n}x_{i}] + \mathbb{E}[x_{n}^2] \right] \\
&= \frac{1}{N} \left[ (N-1)\mu^2 + \mu^2 + \sigma^2 \right] \\
&= \mu^2 + \frac{\sigma^2}{N}
\end{aligned}
$$

Ta ch·ªâ c·∫ßn t√≠nh gi√° tr·ªã c√≤n l·∫°i l√† $\mathbb{E}[\mu_{ML}^2]$. Tr∆∞·ªõc ti√™n ta c·∫ßn bi·∫øt c√¥ng th·ª©c sau:

$$
\left( \sum_{n=1}^N x_{n} \right)^2 = \sum_{n=1}^N x_{n}^2 + \sum_{j=1}^N\sum_{i \neq j}^N x_i x_j
$$

Ch·ª©ng minh n√†y c√¥ng th·ª©c n√†y m√¨nh thua (c√°c b·∫°n c√≥ th·ªÉ xem th√™m ·ªü <d-footnote>https://math.stackexchange.com/questions/329344/what-is-the-square-of-summation</d-footnote>). Sau khi c√≥ c√¥ng th·ª©c r·ªìi th√¨ t√≠nh th√¥i n√†o:

$$
\begin{aligned}
\mathbb{E}[\mu_{ML}^2] &= \mathbb{E}\left[ \frac{1}{N^2} \left( \sum_{n=1}^N x_{n} \right)^2 \right] \\
&= \frac{1}{N^2} \mathbb{E}\left[ \sum_{n=1}^N x_{n}^2 + \sum_{j=1}^N\sum_{i \neq j}^{N} x_{i}x_{j} \right] \\
&= \frac{1}{N^2} \left[ \sum_{n=1}^N \mathbb{E}[x_{n}^2] + \sum_{j=1}^N\sum_{i \neq j}^{N} \mathbb{E}[x_{i}x_{j}] \right] \\
&= \frac{1}{N^2} \left( N(\mu^2 + \sigma^2) + \sum_{j=1}^N\sum_{i\neq j}^{N} \mu^2 \right)
\end{aligned}
$$

·ªû ƒëo·∫°n cu·ªëi, ta th·∫•y nh∆∞ sau:

$$
\begin{aligned}
\sum_{j=1}^N\sum_{i \neq j}^{N} \mu^2 &= (N-1)\mu^2 + ... + (N-1)\mu^2 \hspace{7pt} \text{($N$ l·∫ßn)} \\
&= N(N-1)\mu^2 \\
\end{aligned}
$$

Thay ng∆∞·ª£c v√†o ph∆∞∆°ng tr√¨nh c·ªßa $\mathbb{E}[\mu_{ML}^2]$ ta ƒë∆∞·ª£c:

$$
\begin{aligned}
\mathbb{E}[\mu_{ML}^2] &= \frac{1}{N^2} \left( N(\mu^2 + \sigma^2) + N(N-1)\mu^2 \right) \\
&= \mu^2 + \frac{\sigma^2}{N} = \mathbb{E}[x_{n}\mu_{ML}]
\end{aligned}
$$

Sau khi ƒë√£ c√≥ c·∫£ 3, ta ch·ª©ng minh ƒë∆∞·ª£c, m√¨nh ƒëi ng·ªß ƒë√¢y, d√†i v√£i üíÄ.

$$
\begin{aligned}
\mathbb{E}[\sigma^2_{ML}] &= \frac{1}{N} \sum_{n=1}^N \mathbb{E}[x_{n}^2] -\mathbb{E}[x_{n}\mu_{ML}] \\
&= \frac{1}{N} \sum_{n=1}^N \left( \sigma^2 + \mu^2 - \mu^2 - \frac{\sigma^2}{N} \right) \\
&= \frac{1}{N} \sum_{n=1}^N \left( \frac{N-1}{N} \sigma^2 \right) \\
&= \frac{(N-1)}{N} \sigma^2
\end{aligned}
$$

Sau qu·∫£ tour de force ch·ª©ng minh ph√≠a tr√™n, c√≥ th·ªÉ th·∫•y $\sigma^2_{ML}$ kh√¥ng ƒë∆∞·ª£c nh∆∞ k√¨ v·ªçng l·∫Øm khi n√≥ b·ªã l·ªách ƒëi m·ªôt gi√° tr·ªã $(N-1)/N$, ta c√≥ th·ªÉ g·ªçi ƒë√¢y l√† *bias* khi m√† c·ªë g·∫Øng x·∫•p x·ªâ cho nh·ªØng d·ªØ li·ªáu m√† ta ch∆∞a th·∫•y b·∫±ng m·ªôt l∆∞·ª£ng h·ªØu h·∫°n d·ªØ li·ªáu m√† ta c√≥. V·∫≠y ta mu·ªën lo·∫°i b·ªè bias n√†y, v·∫≠y th·ª≠ nh√¢n k√¨ v·ªçng v·ªõi $N/(N-1)$ xem sao:

$$
\begin{aligned}
\frac{N}{N-1}\mathbb{E}[\sigma^2_{ML}] &= \sigma^2  \\
\mathbb{E}\left[\frac{N}{N-1} \frac{1}{N} \sum_{n=1}^N (x_n - \mu_{ML})^2\right]&= \sigma^2 \\
\mathbb{E}\left[\frac{1}{N-1} \sum_{n=1}^N (x_n - \mu_{ML})^2\right]&= \sigma^2
\end{aligned}
$$

V·∫≠y ƒë·ªÉ t·ªëi ∆∞u nh∆∞ k√¨ v·ªçng, ta ph·∫£i chia cho $N-1$ thay v√¨ $N$ ·ªü ph∆∞∆°ng sai m·∫´u.

{% include figure.liquid class="img-fluid" caption="Reaction c·ªßa m√¨nh" path="https://preview.redd.it/man-im-dead-v0-ymr5u3c0bjsa1.jpg?auto=webp&s=364c87d710ec0cda25a8e23fcbf1dbd692d0a597" %}